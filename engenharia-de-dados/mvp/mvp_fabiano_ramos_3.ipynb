{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otEdveLq8Hn0"
   },
   "source": [
    "# Pós-Graduação em Ciência de Dados e Analytics - PUC-Rio\n",
    "\n",
    "## MVP - Engenharia de Dados\n",
    "\n",
    "### Aluno: Fabiano Ramos (Setembro / 2023)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1PEQEdZ9zm4"
   },
   "source": [
    "## Definição do Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFobRus_SRMb"
   },
   "source": [
    "### Motivação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDcdO4yx9db6"
   },
   "source": [
    "Para este trabalho, utilizamos o conjunto de dados [Airline Dataset](https://www.kaggle.com/datasets/iamsouravbanerjee/airline-dataset), disponibilizado a partir do [Kaggle](https://www.kaggle.com).\n",
    "\n",
    "O conjunto de dados é *artificial*, e oferece informações relacionados a operações aéreas, possibilitando a obtenção de *insights* interessantes sobre a demografia e comportamento dos passageiros. Possui um total de 98619 registros e 15 atributos.\n",
    "\n",
    "Nossas perguntas-alvo foram as seguintes:\n",
    "\n",
    "* Quais os aeroportos mais movimentados?\n",
    "* Quais aeroportos possuem um maior número de atrasos e/ou cancelamentos?\n",
    "* Qual a demografia dos passageiros?\n",
    "\n",
    "O dicionário de dados é o seguinte:\n",
    "\n",
    "*   **Passenger ID** - Identificação única do passageiro.\n",
    "*   **First Name** - Primeiro nome do passageiro.\n",
    "*   **Last Name** - Último nome do passageiro.\n",
    "*   **Gender** - Sexo do passageiro. \n",
    "*   **Age** - Idade do passageiro.\n",
    "*   **Nationality** - Nacionalidade do passageiro.\n",
    "*   **Airport Name** - Nome do aeroporto de partida.\n",
    "*   **Airport Country Code** - Código do país do aeroporto.\n",
    "*   **Country Name** - Nome do país do aeroporto.\n",
    "*   **Airport Continent** - Continente onde o aeroporto está situado.\n",
    "*   **Continents** - Continente envolvido na rota do aeroporto.\n",
    "*   **Departure Date** - Data de decolagem.\n",
    "*   **Arrival Airport** - Aeroporto de destino.\n",
    "*   **Pilot Name** - Nome do piloto.\n",
    "*   **Flight Status** - Situação do vôo (cancelado, atrasado, no horário).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Unjg3qehST3d"
   },
   "source": [
    "### *Stack* de Tecnologia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0O5gyeenSkFA"
   },
   "source": [
    "Decidimos levar a cabo nosso trabalho utilizando a [Amazon Web Services (AWS)](https://aws.amazon.com/).\n",
    "\n",
    "Como ferramenta de integração de dados, utilizamos o [AWS Glue](https://aws.amazon.com/glue/), um *framework* sem servidor destinado a busca, preparação e integração de dados. Utilizamos também o componente [AWS Glue Data Catalog](https://aws.amazon.com/solutions/analytics/data-catalog/) para preparação do Catálogo de Dados correspondente, o [AWS Athena](https://console.aws.amazon.com/athena) para consultas diretas ao catálogo de dados (com dados ainda armazenados no S3) e também o [Amazon RedShift Serverless](https://aws.amazon.com/redshift/), um serviço de *data warehouse*, também sem servidor, a partir do qual realizamos as análises/consultas desejadas.\n",
    "\n",
    "A figura abaixo mostra em linhas gerais nosso *pipeline* de dados:\n",
    "\n",
    "![pipeline](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PcB0Efd-MS4"
   },
   "source": [
    "## Coleta dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iX9s2aWFM5i"
   },
   "source": [
    "### Análise preliminar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxCsHInPFSOq"
   },
   "source": [
    "Em primeiro lugar, realizamos uma inspeção do *dataset* para verificar sua qualidade e para apurar se operações de transformação se fariam necessárias.\n",
    "\n",
    "Fizemos uma [cópia](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/data/Airline_Dataset_Updated.csv) do arquivo *csv* original em nosso github para que este trabalho se mantenha auto-contido e íntegro em caso de consulta futura.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OYGN5G3EJcz4",
    "outputId": "a5b07612-486f-4792-b0f2-a9b6ffcc1369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98619, 15)\n",
      "Passenger ID            0\n",
      "First Name              0\n",
      "Last Name               0\n",
      "Gender                  0\n",
      "Age                     0\n",
      "Nationality             0\n",
      "Airport Name            0\n",
      "Airport Country Code    0\n",
      "Country Name            0\n",
      "Airport Continent       0\n",
      "Continents              0\n",
      "Departure Date          0\n",
      "Arrival Airport         0\n",
      "Pilot Name              0\n",
      "Flight Status           0\n",
      "dtype: int64\n",
      "['Female' 'Male']\n",
      "['On Time' 'Delayed' 'Cancelled']\n"
     ]
    }
   ],
   "source": [
    "# carga das bibliotecas\n",
    "import pandas as pd\n",
    "\n",
    "# desabilita warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# carrega o dataset\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/data/Airline_Dataset_Updated.csv')\n",
    "\n",
    "# dimensoes do dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# nulos?\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "# atributos categóricos estão corretos?\n",
    "print(dataset['Gender'].unique())\n",
    "print(dataset['Flight Status'].unique())\n",
    "\n",
    "# ID é mesmo único?\n",
    "assert(dataset['Passenger ID'].nunique() == dataset.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fATb1VQeMYLz"
   },
   "source": [
    "Como podemos observar, o conjunto de dados é bem comportado (inclusive os atributos categóricos *Flight Status* e *Gender*) e pode ser carregado diretamente sem necessidade de pré-processamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBjX6HGRFXTC"
   },
   "source": [
    "### Carga dos Dados para a Nuvem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4XWdue7_MVFH"
   },
   "source": [
    "Para a coleta dos dos dados, criamos um script ETL python, a partir do AWS Glue.\n",
    "\n",
    "O script é responsável por buscar os [dados](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/data/Airline_Dataset_Updated.csv) diretamente do nosso github e salvá-los dentro de um bucket S3 previamente criado.\n",
    "\n",
    "O passo a passo da implantação do script foi o seguinte:\n",
    "\n",
    "1.   Criação de um IAM Role para execução. Criamos um Role chamado *AWSGlue*, composto pelas políticas pré-definidas *AWSGlueServiceRole*, *AWSGlueSchemaRegistryFullAccess* e *AWSGlueConsoleFullAccess*.\n",
    "\n",
    "![glue-roles](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/glue-roles.png)\n",
    "\n",
    "2.   Criação do S3 bucket destino. Atribuímos o nome *aws-glue-rawdata*, uma vez que as permissões do *AWSGlueServiceRole* exigem que o nome do bucket possua *aws-glue-* como prefixo.\n",
    "\n",
    "3.   Criação do script python propriamente dito, que chamamos de *data-collect*, conforme mostramos a seguir. O [script](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/scripts/data-collect.py) também pode ser consultado em nosso github.\n",
    "\n",
    "![data-collect](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/data-collect.png)\n",
    "\n",
    "Através da execução do script, os dados de entrada foram então coletados e armazenados no S3 bucket indicado.\n",
    "\n",
    "![aws-glue-rawdata](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/aws-glue-rawdata.png)\n",
    "\n",
    "Finalmente, nas configurações de rede, criamos um *endpoint* para que o S3 pudesse ser acessado pelos jobs ETL Glue mais a frente, durante a carga dos dados para o *data warehouse*.\n",
    "\n",
    "![s3-endpoint](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/s3-endpoint.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE4-PIaTAfKX"
   },
   "source": [
    "## Modelagem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySvgVcC4Kwdk"
   },
   "source": [
    "Decidimos então manter a representação dos dados em sua forma *flat*, como uma única tabela, como em um Data Lake.\n",
    "\n",
    "Prosseguimos então para a definição do Catálogo de Dados associado, usando o [AWS Glue Data Catalog](https://aws.amazon.com/solutions/analytics/data-catalog/). Seguimos então os passos descritos a seguir:\n",
    "\n",
    "1. Criação da base de dados *airline-db*.\n",
    "\n",
    "![airline-db](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/airline-db.png)\n",
    "\n",
    "\n",
    "2. Criação da *tabela* , a partir de um *crawler*, com as seguintes configurações:\n",
    "   - Nome do *crawler*: *airline-tbl-crawler*\n",
    "   - Fonte de dados: *s3://aws-glue-rawdata/airline-tbl/*\n",
    "   - IAM role: *AWSGlue*\n",
    "   - Base de dados: *airline-db*\n",
    "\n",
    "![crawler](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/crawler.png)\n",
    "\n",
    "Ao executar o *crawler* a tabela foi criada e seu esquema automaticamente detectado, conforme figura abaixo. O nome da tabela foi automaticamente gerado  a partir do nome da pasta S3 onde o csv estava armazenado, *airline_tbl*  (note que o '-' foi substituído por '_').\n",
    "\n",
    "![datacatalog-table](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/datacatalog-table.png)\n",
    "\n",
    "![datacatalog-schema](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/datacatalog-schema.png)\n",
    "\n",
    "Todos os campos definidos no csv foram corretamente detectados. A seguir, através da opção *Edit schema as JSON*, editamos e ajustamos manualmente o esquema da tabela (nomes de colunas, tipos de dados e comentários), conforme mostramos abaixo. O arquivo [JSON](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/scripts/aws-glue-rawdata.json) completo pode ser encontrado em nosso github. O ajuste do nome das colunas (remoção de espaços) foi fundamental para que a carga para o *data warehouse* (realizada nos passos seguintes) funcionasse corretamente nos passos seguintes.\n",
    "\n",
    "![datacatalog-schema-edited](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/datacatalog-schema-edited.png)\n",
    "\n",
    "\n",
    "Um vez disponibilizado o catálogo de dados, pode-se efetuar consultas diretamente sobre os dados persistidos no S3 utilizando o componente [AWS Athena](https://aws.amazon.com/athena/). Neste momento realizamos então apenas uma consulta básica com o objetivo de verificar que o catálogo de dados foi criado corretamente.\n",
    "\n",
    "Para a correta utilização do Athena, um bucket s3 precisou ser configurado para armazenar os resultados das consultas. Criamos então um bucket chamado *aws-glue-athena-workspace* para tal propósito.\n",
    "\n",
    "A seguir, realizamos uma consulta simples, para verificar a integridade do catálogo de dados:\n",
    "\n",
    "![athena-query](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/athena-query.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REo1UJQZLuFV"
   },
   "source": [
    "## Carga dos Dados no Data Warehouse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VmCYUtv4Qdt"
   },
   "source": [
    "\n",
    "\n",
    "O passo seguinte foi então carregar os dados no [AWS Redshift Serverless](https://us-east-1.console.aws.amazon.com/redshiftv2/home?region=us-east-1#landing).\n",
    "\n",
    "Realizamos então a configuração inicial do serviço. Criamos um IAM Role apropriado contendo a política *AmazonRedshiftAllCommandsFullAccess*.\n",
    "\n",
    "![redshift-creation-1](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/redshift-creation-1.png)\n",
    "\n",
    "![redshift-creation-2](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/redshift-creation-2.png)\n",
    "\n",
    "Neste momento, voltamos ao Glue Studio para a criação de um job ETL tendo como fonte de dados o Catalógo de Dados previamente criado e, como destino, o Redshift.  Como pré-requisito, foi necessário configurar uma conexão com o RedShift, conforme mostrado abaixo.\n",
    "\n",
    "![redshift-connection](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/redshift-connection.png)\n",
    "\n",
    "Uma vez criada a conexão, o job ETL pôde então ser criado, conforme mostrado abaixo. Criamos o job visualmente, mas o [fonte completo](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/scripts/load-redshift.py) do script pode ser observado em nosso github.  Não aplicamos nenhuma transformação de tipos de dados no esquema, mas descartamos os campos *First Name*, *Last Name* e *Pilot Name* que são irrelevantes ao problema.\n",
    "\n",
    "![redshift-load](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/redshift-load.png)\n",
    "\n",
    "Após a carga, realizamos então uma consulta de testes a partir do console do RedShift, no sentido de validar a carga:\n",
    "\n",
    "![redshift-query](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/redshift-query.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoLQBjKl9JVD"
   },
   "source": [
    "## Análise dos Dados\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4uYCXt4Lq8u"
   },
   "source": [
    "### Qualidade dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVtf5ySGLtnn"
   },
   "source": [
    "Já havíamos realizado a inspeção do *dataset* durante a fase preliminar e não encontramos problemas estruturais.\n",
    "\n",
    "Notamos, no entanto, que as informações sobre os aeroportos de chegada (código) não são compatíveis (em nível de detalhe) com as dos aeroportos de partida (nome/país), o que limita bastante as oportunidades de análise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_tIMA6YLvT8"
   },
   "source": [
    "### Solução das Questões Propostas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-doBIHqUL0mL"
   },
   "source": [
    "#### Quais os aeroportos mais movimentados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dng0vQKPiLqS"
   },
   "source": [
    "A primeira pergunta que tentamos responder foi determinar os aeroportos mais movimentados.  \n",
    "\n",
    "Realizamos então uma consulta relativamente simples, com o totalizador e deixando de fora os vôos cancelados. Limitamos os resultados a apenas os 10 aeroportos mais movimentados.\n",
    "\n",
    "A consulta e sua resposta são mostrados na figura abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0_pKYWsOI-K"
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "    airport_name, airport_country_code, COUNT(*) AS total\n",
    "FROM\n",
    "    airline\n",
    "WHERE\n",
    "    flight_status != 'Cancelled'\n",
    "GROUP BY\n",
    "    airport_name, airport_country_code\n",
    "ORDER BY\n",
    "    total DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAONBQ4vevbP"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "![query-movimentados](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/query-movimentados.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1TwP5EyewB3"
   },
   "source": [
    "#### Quais aeroportos possuem um maior número de atrasos e/ou cancelamentos?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rExlqpeDirYh"
   },
   "source": [
    "A segunda pergunta que tentamos responder foi determinar os aeroportos mais afetados por atrasos e/ou cancelamentos.\n",
    "\n",
    "Um fator complicador para a consulta foi que tentamos arbitrariamente filtrar apenas os aeroportos com 20 ou mais vôos, já que este tipo de consulta estatística faz pouco sentido para aeroportos muito pequenos. Este pequeno detalhe fez com que precisássemos realizar uma subconsulta, uma vez que a cláusula WHERE não poderia ser inserida na primeira *query*.\n",
    "\n",
    "Mais uma vez, limitamos os resultados a apenas os 10 aeroportos mais afetados. \n",
    "\n",
    "A consulta e sua resposta são mostrados na figura abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWo09xPeOl0r"
   },
   "outputs": [],
   "source": [
    "WITH flight_problems AS (\n",
    "    SELECT\n",
    "        airport_name, airport_country_code,\n",
    "        SUM(CASE WHEN flight_status = 'Cancelled' OR flight_status = 'Delayed' THEN 1 ELSE 0 END) AS problems,\n",
    "        COUNT(*) AS total, problems*100.0/total AS percentual\n",
    "    FROM\n",
    "        airline\n",
    "    GROUP BY\n",
    "        airport_name, airport_country_code\n",
    ")\n",
    "SELECT\n",
    "    airport_name, airport_country_code, problems, total, percentual\n",
    "FROM\n",
    "    flight_problems\n",
    "WHERE\n",
    "    total > 20\n",
    "ORDER BY\n",
    "    percentual DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvxwM2tfeySj"
   },
   "source": [
    "![query-percentual](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/query-percentual.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0ngdfwhe0lD"
   },
   "source": [
    "#### Qual a demografia dos passageiros?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6gJC6AZkHPA"
   },
   "source": [
    "Para a terceira pergunta, tentamos entender a  demografia dos passageiros.\n",
    "\n",
    "Idosos e crianças trazem requisitos diferenciados às empresas aéreas (preço e acomodações) então consideramos esta uma consulta relevante. Incluímos também o sexo dos passageiros, e decidimos agrupar por continente para uma melhor representatividade.\n",
    "\n",
    "A consulta e sua resposta são mostrados na figura abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "koSwZJR2POx2"
   },
   "outputs": [],
   "source": [
    "SELECT\n",
    "    airport_continent,\n",
    "    SUM(CASE WHEN Gender = 'Male' THEN 1 ELSE 0 END) AS males,\n",
    "    SUM(CASE WHEN Gender = 'Female' THEN 1 ELSE 0 END) AS females,\n",
    "    SUM(CASE WHEN Age > 65 THEN 1 ELSE 0 END) AS seniors,\n",
    "    SUM(CASE WHEN Age < 12 THEN 1 ELSE 0 END) AS children,\n",
    "    COUNT(*) AS total\n",
    "FROM\n",
    "    airline\n",
    "GROUP BY\n",
    "    airport_continent\n",
    "ORDER BY\n",
    "    airport_continent;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n_zwZ1Re23L"
   },
   "source": [
    "![query-distribuicao](https://raw.githubusercontent.com/pragmaerror/puc-ciencia-de-dados-e-analytics/main/engenharia-de-dados/mvp/images/query-distribuicao.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n8VMRNvXUMV"
   },
   "source": [
    "## Conclusão & Auto Avaliação\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC_ZuXPQ6NMz"
   },
   "source": [
    "As atividades aqui descritas correspondem ao nosso primeiro contato com a atividade de ETL (e Engenharia de Dados de uma forma mais ampla).\n",
    "\n",
    "Nosso foco pessoal foi o aprendizado da platforma AWS, e desta forma tentamos manter as demais variáveis (*dataset* e escopo do trabalho) o mais controladas possível dado o curto prazo de confecção desde projeto.  Já tínhamos alguma familiaridade com o GCP (embora em outro contexto) e nos desafiamos a aprender a AWS, o que foi bastante recompensador. \n",
    "\n",
    "Neste trabalho, fomos capazes de produzir o *pipeline* desejado, desde a análise preliminar dos dados, passando pela sua (simples) modelagem, produção do catálogo de dados correspondente até a sua carga no *data warehouse*, a partir de onde realizamos as consultas desejadas.\n",
    "\n",
    "Como possibilidades de continuação deste trabalho, podemos destacar em primeiro lugar melhorar a reprodutibilidade da construção do *pipeline*,  uma vez que realizamos muitos passos manuais. Entendemos que técnicas de IaC (*infrastructure as code*) seriam indispensáveis em um ambiente de produção. Mesmo dentro do AWS Glue, identificamos a funcionalidade de *workflows*, que supostamente permitem a definição do fluxo de trabalho através de *blueprints*. Seria bem interessante explorar este caminho no futuro.\n",
    "\n",
    "Outro desenvolvimento que gostaríamos de realizar no futuro é a interação programática (via *python*) com o AWS Glue e com o Redshift.\n",
    "\n",
    "Além disso, o uso de dados reais como por exemplo os [dados abertos](https://www.anac.gov.br/acesso-a-informacao/dados-abertos/) da ANAC possibilitariam mais oportunidades de modelagem, transformação e análise. Chegamos examinar estes dados, mas eles apresentaram demasiados problemas e inexatidões de forma que preferimos evitá-los neste momento, por razões já apresentadas.\n",
    "\n",
    "Finalmente, de forma geral nossa avaliação desta atividade foi bastante positiva. O escopo bastante amplo, ao mesmo tempo que foi um ponto de complexidade adicional, permitiu aos estudantes direcionar suas implementações de acordo com seus interesses e competências individuais.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "V100",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
